{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focusstacking in PYNQ on Xilinx Kria KV260\n",
    "***\n",
    "Version: V1.1 as of 29.03.2022\n",
    "Author: Andreas Rudolph as https://github.com/produkt-manager/focusstacking\n",
    "\n",
    "Helper documentation\n",
    "- Markdown cheat sheet https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet\n",
    "- 2019 (current version) https://www.xilinx.com/content/dam/xilinx/support/documentation/sw_manuals/xilinx2019_1/ug1233-xilinx-opencv-user-guide.pdf\n",
    "- This describes how to work with user defined functions https://notebook.community/evanmiltenburg/python-for-text-analysis/Chapters/Chapter%2011%20-%20Functions%20and%20scope\n",
    "\n",
    "\n",
    "This notebook is inspired by/ reusing https://github.com/cmcguinness/focusstack (licensed as by https://www.apache.org/licenses/LICENSE-2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How To use this Notebook\n",
    "***\n",
    "This Notebook uses PYNQ on a KV260, and employs OpenCV in order to implement its imaging features. Perform these steps to run the notebook, after you have uploaded it to your board:\n",
    "\n",
    "1. Use your own focal stack, or download images from Wikipedia https://de.wikipedia.org/wiki/Focus_stacking\n",
    "2. Create the following folders in the PYNQ file system on your XILINX KRIA KV260 in order to be able to store the images in the file system\n",
    "    - \"inbound_images\" \n",
    "    - \"outbound_images\"\n",
    "    - \"test_images\"\n",
    "    - \"temp_images\"\n",
    "\n",
    "3. Store the images to stack in the folder __\"inbound_images\"__. The first image of your images to stack will serve as a reference image. All other images will be the stack and they relate to this reference.\n",
    "4. Run the notebook, and alter the parameters to fit your needs\n",
    "5. The notebook produces a stacked image, and stores it in the folder __\"inbound_images\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definitions and Preparations\n",
    "***\n",
    "Import libraries, set all paths, and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import glob, os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Set Debug Mode\n",
    "\n",
    "Set this switch to true, if you want to have a log of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging = True\n",
    "# debugging = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Set Path to Images\n",
    "\n",
    "Define where to find the images in the file system of your installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2inimages = \"inbound_images\"  # Input of raw-Images to be stacked \n",
    "path2outimages = \"outbound_images\" # Output of stacked image\n",
    "path2testimages = \"test_images\" # Test images\n",
    "path2tempimages = \"temp_images\" # Intertemporal results\n",
    "\n",
    "delimiter = \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Settings for the Perspective Transformation Process in Image Alignment\n",
    "\n",
    "The homography, and it's corresponding function in OpenCV represents a perspective transformation between two planes. This function is needed to align the different images of a focus stack to the reference image.\n",
    "\n",
    "The process is defined as following (see https://en.wikipedia.org/wiki/Homography_(computer_vision):\n",
    "\n",
    "> _\"In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or camera motion—rotation and translation—between two images. Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).\"_ \n",
    "\n",
    "OpenCV's homography method is defined as\n",
    "\n",
    "> _ \"openCV cv.findHomography(srcPoints, dstPoints[, method[, ransacReprojThreshold[, mask[, maxIters[, confidence]]]]])\n",
    "\n",
    "and it returns a returnvalue __\"retval'__, as well as a __\"mask\"__.\n",
    "\n",
    "In this method OpenCV offers different calculations for the homography. Set the corresponding switch in order to define, which method is used in this notebook. \n",
    "\n",
    "> _ Methods 0 - a regular method using all the points, i.e., the least squares method\n",
    "\n",
    "> _ RANSAC - RANSAC-based robust method\n",
    "\n",
    "> _ LMEDS - Least-Median robust method\n",
    "\n",
    "> _ RHO - PROSAC-based robust method\n",
    "\n",
    "The parameter __\"ransacReprojThreshold\"__ defines, the threshold as of which a point i is considered as an outlier. The sourcepoints __\"srcPoints\"__ and the destination points __\"dstPoints\"__ are measured in pixels. Usually these parameters are set in a range of 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method to use. To do so, set one parameter to true, and the rest to false\n",
    "\n",
    "setRANSAC = True # RANSAC - RANSAC-based robust method\n",
    "setLMEDS = False # LMEDS - Least-Median robust method\n",
    "setRHO = False # RHO - PROSAC-based robust method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Settings for the Image Alignment Method\n",
    "\n",
    "As you can use different methods for image alignment you can set here the corresponding switches _(Remember that gosting in your output image might indicate that images are missaligned, and this might be the result of either the method, or the process parameters.)_\n",
    "\n",
    "* SIFT (Scale-Invariant Feature Transform), as described here https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html. This method goes back to __\"D.Lowe, University of British Columbia (2004)\"__, who came up with a new algorithm that is able to extract keypoints and compute its descriptors, as needed by image scaling transactions. This algorithm is covered by patents.\n",
    "\n",
    "* ORB (Oriented FAST and Rotated BRIEF) keypoint detector and descriptor extractor as described here https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html. ORB was created by Rublee, et.al in their 2011'er paper __\"ORB: An efficient alternative to SIFT or SURF\"__. The algorithm is an alternative to SIFT and SURF in terms of computation cost and matching performance. While SIFT and SURF are covered by patents, ORB is open Source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Set one alignment method to true, the rest to false\n",
    "setSIFT = True\n",
    "setORB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Settings for the LoB Filter (Laplacian Blur)\n",
    "\n",
    "Focus stacking requires that those parts of an image are identified, that appear to be in-focus, while out-of-focus areas are neglected. \n",
    "\n",
    "The parameters for the __\"size of laplacian kernel window\"__, and the __\"size of the kernal that is used for the gaussian blur\"__ allow you to influence, which areas are considered as sharp in an image. You can alter and tune these parameters, so that they match your needs. \n",
    "\n",
    "In order to obtain good results,  these parameters typically should be set to identical in value, or they should at least fall into the same range of values.\n",
    "\n",
    "Due to the functional principles of the involved algorithms, make sure that you only choose odd numers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switches for the LoB filter\n",
    "\n",
    "kernel_size = 5         # Size of the laplacian kernel window\n",
    "blur_size = 5           # Size of the kernal that is used for the gaussian blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import and Show the Image Objects in Frontend\n",
    "***\n",
    "The current version of the notebook is assuming that user takes a stack of images, and places them in the import image folder. In the current version, fotos of format <> JPG, or PNG are not processed, and therefore removed before the import without further notice.\n",
    "\n",
    "This section reads the images from the file system into one list. This notebook is assuming, that all photos found within this folder need to be stacked. Photo [0] will thereby be considered as the reference-image. \n",
    "\n",
    "In a future version of this notebook, the images-to-be-stacked might originate from a camera module that is attached to the board, and a focus rail that is driven by the same board. For such tasks the Composable Pipeline Overlay in the standard PYNQ installation is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Read the Image Names from the Import Folder\n",
    "\n",
    "Create a list of images to be read/ imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image names into a list and skip files in the wrong format\n",
    "\n",
    "inbound_image_names  = sorted(os.listdir(path2inimages))\n",
    "\n",
    "if debugging:  \n",
    "    print (\"Name of files stored in image path {}\".format(inbound_image_names))\n",
    "\n",
    "# Test for wrong formats and remove from filelist\n",
    "for inbound_image_name in inbound_image_names:\n",
    "    if inbound_image_name.split(\".\")[-1].lower() not in [\"jpg\", \"jpeg\", \"png\"]:\n",
    "        if debugging:  \n",
    "            print (\"Remove a file that has a wrong format {}\".format(inbound_image_name))\n",
    "        inbound_image_names.remove(inbound_image_name)\n",
    "\n",
    "# Alternative\n",
    "# file, ext = os.path.splittext(inbound_image_name)\n",
    "# if ext not in [\"jpg\", \"jpeg\", \"png\"]:\n",
    "#        if debugging:  \n",
    "#            print (\"Remove file with wrong format {}\".format(inbound_image_names))\n",
    "#        inbound_image_names.remove(inbound_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read images into the List \"original_images\"\n",
    "\n",
    "Import images from file system, and create thumbnails.\n",
    "\n",
    "The create thumbnails step is optional and it is helpful, in order to make sure that images are available for presentation purposes that are smaller in size. Alternatively you can use image pyramids. This describes how to create an image pyramid: \n",
    "https://docs.opencv.org/3.4/d4/d1f/tutorial_pyramids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images into a list of original_images\n",
    "\n",
    "original_images = [] # images\n",
    "original_images_thumb = [] # thumbnails\n",
    "\n",
    "for inbound_image_name in inbound_image_names:\n",
    "    if debugging:\n",
    "        print (\"Reading original image with name {}\".format(inbound_image_name))\n",
    "\n",
    "# OpenCV-read one photo, output size, and append to the list of original_images\n",
    "    one_original_image = cv2.imread(path2inimages + delimiter + \"{}\".format(inbound_image_name))\n",
    "    if debugging:\n",
    "        one_original_image_width, one_original_image_height = one_original_image.size\n",
    "        print(\"Original image with size: {}x{} pixels.\".format(one_original_image_width, one_original_image_height))\n",
    "    original_images.append(one_original_image)\n",
    "\n",
    "    # Create thumbnails and store in separate path\n",
    "    tn_size = 128, 128\n",
    "    one_original_image_thumb = Image.fromarray(np.asarray(one_original_image))\n",
    "    one_original_image_thumb.thumbnail(tn_size)\n",
    "    if debugging:\n",
    "        one_original_image_thumb_width, one_original_image_thumb_height = one_original_image_thumb.size\n",
    "        print(\"Original thumbnail with size: {}x{} pixels.\".format(one_original_image_thumb_width, one_original_image_thumb_height))\n",
    "    one_original_image_thumb.save(path2tempimages + delimiter  + \"thumbnail\" + \"{}\".format(inbound_image_name), \"JPEG\")   \n",
    "    original_images_thumb.append(one_original_image_thumb)\n",
    "\n",
    "if debugging:\n",
    "    print (\"Number of images read from folder original_images {}\".format(len(original_images)))\n",
    "    print (\"Number of images converted into thumbnails {}\".format(len(original_images_thumb)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create a Copy of List \"images_2_stack\"\n",
    "\n",
    "Copy the list named __\"original_images\"__ to a list named __\"images_2_stack\"__. __\"Images_2_stack\"__ will lateron be processed with focus stacking, and this step allows you to run a before-after comparision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_2_stack = original_images.copy()\n",
    "\n",
    "if debugging:\n",
    "    print (\"Number images in images_2_stack list {}\".format(len(images_2_stack)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Display Images in Notebook Frontend\n",
    "\n",
    "After we have imported all images, we now show all images in the frontend. For this task we use the standard pillow library and therefore it is necessary to color convert images between the color format for OpenCV, and for Pillow.\n",
    "\n",
    "As you can verify from the output, the list __\"original_images\"__ includes a stack of images with varying focus planes. Observe how the focus plane shifts from image to image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = plt.gcf()\n",
    "size = canvas.get_size_inches()\n",
    "canvas.set_size_inches(size*2)\n",
    "\n",
    "# axis off\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# create grid\n",
    "concat_original_images = np.concatenate(original_images, axis=1)\n",
    "if debugging:\n",
    "    print (\"Number of images in OpenCV color space {}\".format(len(original_images)))\n",
    "# wird überschrieben ---> anpassen \n",
    "    plt.imshow(concat_original_images)\n",
    "\n",
    "# Color Convert, as OpenCV imread() has order of colors BGR (blue, green, red), but Pillow assumes RGB. \n",
    "concat_original_images_RGB = cv2.cvtColor(concat_original_images, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(concat_original_images_RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Focus Stacking of imported Images\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having imported the images, they can now be stacked. Basic idea/ algorithm: From the different images identify the points that are in focus and merge theses points into a common image.\n",
    "\n",
    "These are the high level processing steps covered in this part of the notebook:\n",
    "* Align the individual images that define the stack (thereby one image saves as reference)\n",
    "* Apply a lapacian blur filter. This filter is used to identify the sharpest points in an image\n",
    "* Create the common image from the single images of the stack, by using the absolute value of the LoB filter as a criterion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Align Images that belong to the Focal Stack\n",
    "\n",
    "Due to micromovements of the camera inbetween shots, or due to a moving object that is captured, it is possible that the individual images of the stack are displaced. Therefore it is first necessary to align all images to one reference image, and to make sure that they overlap properly.\n",
    "\n",
    "Wrongly aligned images may lead to ghosting in the final stacked image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. Define the Detector Functions\n",
    "\n",
    "Preparation of image alignment, either using the SIFT or the ORB process as described above.\n",
    "\n",
    "##### SIFT\n",
    "\n",
    "This notebook uses the Scale Invariant Feature Transform (SIFT) class for extracting keypoints and computing descriptors as described above. The creation method is defined as\n",
    "\n",
    "> _\"cv.SIFT_create([, nfeatures[, nOctaveLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]])\"_ \n",
    "\n",
    "According to the documentation, these are the parameters:\n",
    "\n",
    "> _- \"nfeatures: The number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast)\"_\n",
    "\n",
    "> _- \"nOctaveLayers: The number of layers in each octave. 3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.\"_\n",
    "\n",
    "> _- \"contrastThreshold:\tThe contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\"_\n",
    "\n",
    "##### ORB\n",
    "\n",
    "The class that is implementing the ORB keypoint detector and descriptor extractor, is described here https://docs.opencv.org/3.4/db/d95/classcv_1_1ORB.html\n",
    "\n",
    "According to the documentation, the algorithm detects stable keypoints by means of using FAST in pyramids. Then it selects the strongest features either using FAST or using the Harris response. It finds the orientation of the strongest features using first-order moments. The algorithm then computes the descriptors using a method in which the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_images_2_stack = []\n",
    "\n",
    "if setSIFT:\n",
    "    detector = cv2.xfeatures2d.SIFT_create()\n",
    "elif setORB:\n",
    "    detector = cv2.ORB_create(1000)\n",
    "else:\n",
    "    if debugging:\n",
    "        print (\"ERROR with setting either the ORB or the SIFT detector - should not happen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Detect Keypoints and Compute the Descriptors for the Reference Image\n",
    "\n",
    "In order to align the images-to-be-stacked, we identify keypoints in each image. Afterwards we move the images in a way that these keypoints are located at the same location. Thereby the first image in the stack acts as the \"base\" or reference image.\n",
    "\n",
    "The reference image is prepared as follows:\n",
    "\n",
    "* OpenCV offers the __\"DetectAndCompute\"__ method of the detector class for the keypoint-detection-task. This method in particular detects keypoints and computes their descriptors. It returns the keypoints and the descriptors of the image, and is defined as __\"cv.Feature2D.detectAndCompute(\timage, mask[, descriptors[, useProvidedKeypoints]])-> keypoints, descriptors\"__\n",
    "\n",
    "* For best results, we convert the color image to greyscale using OpenCV's function __\"cvtColor\"__ that accepts the image to be converted, and a setting that defines that we will convert the colors BGR (blue, green, red) to grey (*Remark: OpenCV uses the colors BGR (blue, green, red) color order, while Pillow assumes RGB*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect keypoints and compute the descriptors\n",
    "\n",
    "if debugging:\n",
    "    print (\"Detecting features of base image that can ten be used for alignment\")\n",
    "\n",
    "aligned_images_2_stack.append(images_2_stack[0])  # Reference image\n",
    "reference_image_in_gray = cv2.cvtColor(images_2_stack[0],cv2.COLOR_BGR2GRAY)\n",
    "reference_image_kp, reference_image_desc = detector.detectAndCompute(reference_image_in_gray, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3. Align Images by Means a Brute-Force Descriptor Matcher, Homography-Finder and Image Warping \n",
    "\n",
    "The alignment process consists of the following steps\n",
    "* Detect keypoints\n",
    "* Using a brute-force descriptor matcher the topmost matches for the keypoints of the image is returned, and the matches are paired.\n",
    "* Using a homography algorithmn the matrix of of aligned image points is calculared\n",
    "* Using these matrices, the images are warped so that the rlevant keypoints are located at identical positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Key Point Detection\n",
    "\n",
    "Using the same __\"detectAndCompute\"__ method as described above, each of the non-reference images in the list of images-to-be-stacked is converted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Brute Force Matcher\n",
    "\n",
    "Using the resulting matrix of identified keypoints, a brute-force descriptor matcher is used to idenfify matching points within the images (reference image, and image-in-process). As mentioned above, OpenCV offers different Brute-force descriptor matcher methods, depending on the fact, which processing method is used (SIFT or ORB).\n",
    "\n",
    "* __Brute-force descriptor matcher for SIFT__: For each descriptor in the first set, this matcher finds the closest \n",
    "descriptor in the second set by trying each descriptor. It is possible to mask permissible matches of descriptor sets, and make sure that the methid applies to selected areas only. \n",
    "\n",
    "* __Brute-force descriptor matcher for ORB__: The matcher is called with two sets. For each descriptor in the first set, it finds the closest descriptor in the second set. He does so, by trying each one. This descriptor matcher as well supports masking. The matcher is as well called with a normtype, which defines the calculation method.\n",
    "\n",
    "The create method of the BFMatcher sets the norm type, and a crosscheck-flag. Both define, how the matcher behaves. The optimal normtype depends on the used method. The documentation defines the optimal use as this:\n",
    "\n",
    "> \"normType: One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are preferable choices for SIFT and SURF descriptors,NORM_HAMMING should be used with ORB, BRISK and BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4.\"\n",
    "\n",
    ">\"crossCheck: If it is false, this is will be default BFMatcher behaviour when it finds the k nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\"\n",
    "\n",
    "The current notebook uses the Hamming norm. If the __NORM_HAMMING__ parameter is used with one input array, it calculates the Hamming distance of the array from zero. If this parameter is used with two input arrays, the Hamming distance between these arrays is calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Pair Matches\n",
    "\n",
    "After matches are found with the brute-force descriptor matcher, the __\"knnMatch\"__ method is used to pair these matches, and to identify the two topmost matches. Only those pairs below a certain threshold are returned.\n",
    "\n",
    "This returns the top two matches for each feature point (list of list)\n",
    "\n",
    "The method is defined as __\"cv.DescriptorMatcher.knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) ->matches\"__, while k defines the number of best matches that is found with each query descriptor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D) Homography\n",
    "As described above, OpenCV's homography finder, finds the information that is needed to apply a perspective transformation between two planes. This function allows us to align the different images of a focus stack to the reference image.\n",
    "\n",
    "* reference_image_points -> Keypoints from reference photo\n",
    "* stacked_images_points -> Keypoints from photos to stack\n",
    "* matched points (Best match for each descriptor from a query set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E) Warp Image\n",
    "\n",
    "After Homography is calculated, the images are warped and undergo a perspective transformation.\n",
    "\n",
    "The notebook uses the __warpPerspective()__ method in order to apply a perspective transformation to each image (see documentation here https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87)\n",
    "\n",
    "To do so, when the flag WARP_INVERSE_MAP is set, the __warpPerspective__ function transforms the source image using the specified matrix. If the parameter is not set, the transformation is first inverted with invert and then put in the formula above instead of M. The function cannot operate in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Image Alignment main loop\n",
    "\n",
    "for i in range(1,len(images_2_stack)):\n",
    "    if debugging:\n",
    "        print (\"Aligning image {}\".format(i))\n",
    "    # Keypoint detection\n",
    "    stacked_images_kp, stack_image_desc = detector.detectAndCompute(images_2_stack[i], None)\n",
    "\n",
    "    # Brute-force descriptor matching\n",
    "    if setSIFT:        \n",
    "        bf = cv2.BFMatcher()\n",
    "        pairMatches = bf.knnMatch(stack_image_desc,reference_image_desc, k=2)\n",
    "\n",
    "        # Select only certain matches by distance\n",
    "        rawMatches = []\n",
    "        for m,n in pairMatches:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                rawMatches.append(m)\n",
    "    elif setORB:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        #  match cv.DescriptorMatcher.match(\tqueryDescriptors, trainDescriptors[, mask]\t) ->\tmatches. \n",
    "        # Find the best match for each descriptor from a query set.\n",
    "        rawMatches = bf.match(stack_image_desc, reference_image_desc)\n",
    "\n",
    "    # Final processing for the matches (sort rawMatches array by distance)\n",
    "    sortMatches = sorted(rawMatches, key=lambda x: x.distance)\n",
    "    matches = sortMatches[0:128]\n",
    "\n",
    "   # -----> H O M O G R A P H Y \n",
    "\n",
    "    # Initialize the needed arrays\n",
    "    stacked_images_points = np.zeros((len(matches), 1, 2), dtype=np.float32)\n",
    "    reference_image_points = np.zeros((len(matches), 1, 2), dtype=np.float32)\n",
    "\n",
    "    # As example shows: np.zeros((5,), dtype=int) -> array([0, 0, 0, 0, 0]) --> array filled with zeros of datatype int\n",
    "    for i in range(0,len(matches)):\n",
    "        stacked_images_points[i] = stacked_images_kp[matches[i].queryIdx].pt\n",
    "        reference_image_points[i] = reference_image_kp[matches[i].trainIdx].pt\n",
    "\n",
    "    # Selected method (see above settings)\n",
    "    if setRANSAC:\n",
    "        the_homography, mask = cv2.findHomography(stacked_images_points, reference_image_points, cv2.RANSAC, ransacReprojThreshold=2.0)\n",
    "    elif setLMEDS:\n",
    "        # ransacReprojThreshold does not work with LMEDS\n",
    "        the_homography, mask = cv2.findHomography(stacked_images_points, reference_image_points, cv2.LMEDS)\n",
    "    elif setRHO:\n",
    "        the_homography, mask = cv2.findHomography(stacked_images_points, reference_image_points, cv2.RHO, ransacReprojThreshold=2.0)\n",
    "    else:\n",
    "        print (\"ERROR with setting homography method\")\n",
    "\n",
    "    # Warp Images to match the points found in homography\n",
    "    warpedimage = cv2.warpPerspective(images_2_stack[i], the_homography, (images_2_stack[i].shape[1], images_2_stack[i].shape[0]), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Store warped image in result list   \n",
    "    aligned_images_2_stack.append(warpedimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compute the Gradient Map of the aligned Images\n",
    "\n",
    "Once the images-to-be-stacked are aligned to each other, the gradient maps are calculated. Gaussian blur is added so that pixels in the same focus plane appear to be lighted identically.\n",
    "\n",
    "Thereby the calculated blurred laplacian kernel serves as a measure for the parts of the image that are in focus. The Laplacian of Gaussian (LoG) filter is defined as the Laplace operator applied to a Gaussian kernel. Thereby the LoG is a line detector.\n",
    "\n",
    "The following document describes the needed theoretical background: \n",
    "* https://www.crisluengo.net/archives/1099/\n",
    "* https://rohanchitnis.com/cs194-26/rohan_chitnis_proj3/index.html\n",
    "\n",
    "The amount of the laplacian blur filter depends on a parameter that defines the size of the laplacian window (size of the kernel), and it depends on the part of this kernel that will be gaussian blurred. Variations in these parameters might incluence the result.\n",
    "\n",
    "After the list of images with applied LoB-Filter is available, sufficient information is availalbe to merge those pixels that represent the largest values/ that represent the most sharpest parts of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Gaussian Blur\n",
    "\n",
    "The OpenCV function __gaussianBlur__ applies blur to the source image using a Gaussian filter.\n",
    "\n",
    "The function cuses the specified Gaussian kernel, and kernel size. In-place filtering is as well supported.\n",
    "\n",
    "For more information, see the documentation https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Laplacian\n",
    "\n",
    "The OpenCV function __laplacian__ calculates the Laplacian of the source image.\n",
    "\n",
    "To do so, it either adds the second x and y derivatives, using the Sobel operator (ksize > 1), or it filters the image with a defined 3×3 aperture (ksize == 1). \n",
    "\n",
    "For more information, see the documentation https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6\n",
    "\n",
    "In the current notebook first the Gaussian blur is applied to the image, and then the Laplacian of the blurred image is taken. Therefore this represents the Laplacian Blur filter described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian Blur Filter\n",
    "\n",
    "if debugging:\n",
    "    print (\"Computing the laplacian of the blurred images - LoB Filter\")\n",
    "  \n",
    "laplacian_images = []\n",
    "for i in range(len(aligned_images_2_stack)):\n",
    "    if debugging:\n",
    "        print (\"Laplacian {}\".format(i))\n",
    "        \n",
    "    # Image blurring\n",
    "    a_blurred_image = cv2.GaussianBlur((cv2.cvtColor(aligned_images_2_stack[i],cv2.COLOR_BGR2GRAY), (blur_size,blur_size), 0))\n",
    "\n",
    "    # Laplacian of the blurred image\n",
    "    laplacian_images.append(cv2.Laplacian(a_blurred_image, cv2.CV_64F, ksize=kernel_size))\n",
    "    laplacian_images = np.asarray(laplacian_images)\n",
    "    if debugging\n",
    "        print (\"Shape of array of laplacians = {}\".format(laplacian_images.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Merge the In-Focus-Areas of the Images of the Image Stack\n",
    "\n",
    "These are the steps to merge the sharpest areas within the individual images-to-be-stacked into the final merged image:\n",
    "\n",
    "* The absolute values are idenfified. \n",
    "* The maxima of these values are taken. \n",
    "* A bitwise negation operation is applied in order to collect these largest values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Absolute Value\n",
    "\n",
    "The __\"absolute\"__ function returns an ndarray containing the absolute value of each element in the input array x. For complex input, a + ib, the absolute value is a scalar if x is a scalar.'\n",
    "\n",
    "The element-wise absolute value of an array is calculated as shown in this example\n",
    "\n",
    "x = np.array([-1.2, 1.2])\n",
    "> np.absolute(x)\n",
    "\n",
    "array([ 1.2,  1.2])\n",
    "> np.absolute(1.2 + 1j)\n",
    "\n",
    "1.5620499351813308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Maximum\n",
    "\n",
    "The method __\"ndarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)\"__ returns the maximum values along a given axis (see also numpy.amax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Bitwise not\n",
    "\n",
    "The  bitwise_not() function converts an input array to an output array, by means of a bitwise negation, while it applies a mask. It therefore inverts every bit of an array, and uses the following parameters __\"input array\"__, __\"output array with the same size and type as the input array\"__, __\" an optional operation mask, 8-bit single channel array, that specifies elements of the output array to be changed\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges images after LoB Filter is applied\n",
    "\n",
    "# Prepare image array\n",
    "merged_image = np.zeros(shape=aligned_images_2_stack[0].shape, dtype=aligned_images_2_stack[0].dtype)\n",
    "\n",
    "# Absolute Values\n",
    "abs_laplacian_images = np.absolute(laplacian_images)\n",
    "\n",
    "# Maxima\n",
    "maxima = abs_laplacian_images.max(axis=0)\n",
    "\n",
    "# Mask, including type conversion into uint8\n",
    "bool_mask = abs_laplacian_images == maxima\n",
    "mask = bool_mask.astype(np.uint8)\n",
    "\n",
    "# merged_image is the bitwise_not() over the aligned images applying the mask\n",
    "for i in range(0,len(aligned_images_2_stack)):\n",
    "    merged_image = cv2.bitwise_not(aligned_images_2_stack[i],merged_image, mask=mask[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Store Stacked Image in Filesystem and Show in Frontend\n",
    "\n",
    "Now store final __\"merged_image\"__ into folder at output path, and show __\"merged_image\"__ in frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store images and show it\n",
    "\n",
    "\n",
    "cv2.imwrite(path2outimages + delimiter + \"merged.png\".format(inbound_image_name), merged_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = plt.gcf()\n",
    "size = canvas.get_size_inches()\n",
    "canvas.set_size_inches(size*2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Color Convert, as OpenCV imread() has order of colors BGR (blue, green, red), but Pillow assumes RGB. \n",
    "merged_image_RGB = cv2.cvtColor(merged_image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(merged_image_RGB)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
